##  ResRep: Lossless CNN Pruning via Decoupling Remembering and Forgetting

动物脑中的记忆和遗忘是两个相对独立的过程，由不同的机制和化学物质控制，并不是说我们记住了什么东西就会自然而然地“覆盖”掉什么记忆。



这似乎意味着，如果我们把CNN类比于动物脑，CNN的结构类比于脑结构，CNN的训练（让一些参数变大，一些变小）类比于脑的记忆（一些突触变强，一些变弱），CNN的剪枝（去掉一些结构，如链接、kernel、通道）类比于脑的遗忘（一些神经元和突触的失活），**我们就会想到：让CNN中的不同结构负责“记忆”和“遗忘”应该是有好处的。**



**但是在大多数现有的剪枝方法中，“记忆”和“遗忘”是耦合的。**一方面，“记忆”越强，模型拟合数据能力越强，误差会比较小。另一方面，通过在模型中添加L1或L2惩罚机制，这会促使该模型相应的权重变小，是一种“遗忘”机制。极端情况下，惩罚机制会使模型所有权重变为0，也就是随着“遗忘”加强，模型识别性能可能会越来越差（因为所有权重变为0后，不管输入什么数据，输出的结果都是一样）。



ResRep提出的解决方案是：将原CNN**等价拆分**成负责“记忆”（保持精度不降低）的部分和负责“遗忘”（去掉某些通道）的部分，前者进行“记忆训练”（不改变原目标函数、不改变训练超参、不改变更新规则），后者进行“遗忘训练”（一种基于SGD的魔改更新规则，即**Res**），应该能取得更好的效果（更高压缩率、更少精度损失）。然后，如果我们能将“记忆”和“遗忘”部分**等价合并**成一个更小的模型，不就能实现剪枝了吗？

论文后面关于等价拆分和等价合并的部分没有细细拜读，大致了解了论文的思想。



## DepGraph 通用结构化剪枝

第一部分介绍Torch-Pruning工具，一种通用的结构化剪枝库，并通过实例演示如何快速实现结构化剪枝；第二部分侧重Torch-Pruning的底层算法DepGraph，主要讨论如何建模结构化剪枝中的层依赖，实现任意结构的剪枝。（主要了解第一部分）

想去了解一下什么是结构化剪枝的自动化以及相关模型方法库的使用

####  Torch-Pruning

Torch-Pruning（TP）是一个结构化剪枝库，与现有框架（例如torch.nn.utils.prune）最大的区别在于，TP会物理地移除参数，同时自动裁剪其他依赖层。

DependencyGraph是Torch-Pruning框架的底层算法，它主要作用是“自动寻找耦合层“。

Torch-Pruning库的设计目标就是将这些依赖层的处理完全自动化，帮助我们快速找到与目标层相互耦合的其他层，从实现正确的结构化剪枝。



##  目前工作

大致有了工作内容的眉目

后续我想通过resnet50作为base模型，然后再看看一些好用的模型方法来进行剪枝

不够由于基础知识不是特别的扎实，这周和下周的安排打算先把resnet重新吃透一遍，然后自己手写一个resnet或者用自带的方法库，神经网络这块学习内容还是有点薄弱。之前一直都在读论文，但是都没有去参考对应的代码，有点纸上谈兵的感觉。



